---
title: "Perception of English Voice Quality"
subtitle: "Random Forest Analysis"
format: 
  html:
    toc: true
    code-tools: true
    self-contained: true
    embed-resources: true
execute: 
  code-fold: true
  warning: false
  message: false
author: Mykel Brinkerhoff
date: 2025-08-27 (W)
---

```{r}
#| label: load-packages
#| include: false

# Modeling process packages
library(lme4) # for creating residual H1*
library(rsample) # for resampling procedures
library(caret) # for resampling and model training
library(randomForest) # for tree generation
library(vip) # for feature interpretation
library(ranger) # for performing Random Forest CART analysis

# Helper packages
library(tidyverse) # for data manipulation, graphic, and data wrangling
library(viridis) # for colorblind friendly colors in ggplot
library(here) # for creating pathways relative to the top-level directory
library(reshape2) # for data manipulation
library(Cairo) # for saving the plots as .eps files
library(cowplot) # For creating complex plots
```

```{r}
#| label: adding_data
#| include: false

# Load in the raw vq data at data/raw/VoicesVQ_data.csv
vq_raw <- readr::read_csv(here::here("data", "raw", "VoicesVQ_data.csv"))

# load in the classification data at data/raw/b-c_voice_cluster_labels.csv
vq_classification <- readr::read_csv(here::here(
  "data",
  "raw",
  "b-c_voice_cluster_labels.csv"
))

# Create a variable for colorblind palette
colorblind <- grDevices::palette.colors(palette = "Okabe-Ito")

# Remove unneeded columns
vq_raw <- vq_raw |>
  dplyr::select(
    -c(
      sF0_mean,
      pF0_mean,
      shrF0_mean,
      oF0_mean,
      pF1_mean,
      pF2_mean,
      pF3_mean,
      pF4_mean,
      oF1_mean,
      oF2_mean,
      oF3_mean,
      oF4_mean,
      pB1_mean,
      pB2_mean,
      pB3_mean,
      pB4_mean,
      oB1_mean,
      oB2_mean,
      oB3_mean,
      oB4_mean
    )
  )

# left_join vq_raw with vq_classification by Gender and where Voice = Talker
vq_raw <- vq_raw |>
  dplyr::inner_join(vq_classification, by = c("Talker" = "Voice", "Gender")) |>
  dplyr::rename_at("ClusterCoding", ~"Phonation") |>
  dplyr::relocate(Phonation, .after = Label) |>
  dplyr::select(-c(median_stan_xmouse, cluster_id))

# Adding function for calculating Mahalanobis distance
source(here::here("scripts", "functions", "vmahalanobis.R"))

# create a new dataframe for the cleaned data
vq_clean <- vq_raw

# flag for f0 outliers
vq_clean <- vq_clean |>
  dplyr::group_by(Talker) |>
  dplyr::mutate(
    F0z = (strF0_mean - mean(strF0_mean, na.rm = T)) / sd(strF0_mean)
  ) |>
  dplyr::ungroup()

vq_clean <- vq_clean |>
  dplyr::mutate(f0_outlier = dplyr::if_else(abs(F0z) > 3, "outlier", "OK"))

# Flag for formant outlier
## set distance cutoff for Mahalanobis distance
distance_cutoff <- 6

## calculate mahalnobis distance on formant
vq_clean <- vq_clean |>
  dplyr::group_by(Vowel) |>
  # dplyr::group_by(Talker) |>
  dplyr::do(vmahalanobis(.)) |>
  dplyr::ungroup() |>
  dplyr::mutate(formant_outlier = NA)

## visualize the formant outliers
vq_clean |>
  dplyr::filter(is.na(formant_outlier)) |>
  ggplot2::ggplot(aes(
    x = sF2_mean,
    y = sF1_mean,
    colour = zF1F2 > distance_cutoff
  )) +
  ggplot2::geom_point(size = 0.6) +
  ggplot2::facet_wrap(. ~ Vowel) +
  ggplot2::scale_x_reverse(limits = c(3500, 0), position = "top") +
  ggplot2::scale_y_reverse(limits = c(2000, 0), position = "right") +
  ggplot2::theme_bw()

for (i in 1:nrow(vq_clean)) {
  if (!is.na(vq_clean$zF1F2[i])) {
    if (vq_clean$zF1F2[i] > 6) {
      vq_clean$formant_outlier[i] <- "outlier"
    }
  }
}

## visualize with formant outliers removed
vq_clean |>
  dplyr::filter(is.na(formant_outlier)) |>
  ggplot2::ggplot(aes(
    x = sF2_mean,
    y = sF1_mean
  )) +
  ggplot2::geom_point(size = 0.6) +
  ggplot2::facet_wrap(. ~ Vowel) +
  ggplot2::scale_x_reverse(limits = c(3500, 0), position = "top") +
  ggplot2::scale_y_reverse(limits = c(2000, 0), position = "right") +
  ggplot2::theme_bw()

# flag energy outliers
## convert 0s to NA
vq_clean$Energy_mean[vq_clean$Energy_mean == 0] <- NA

## log10 transform energy
vq_clean <- vq_clean |>
  dplyr::mutate(log_energy = log10(Energy_mean))

# remove f0, formant, and energy outliers
vq_clean <- vq_clean |>
  dplyr::filter(f0_outlier == "OK") |>
  dplyr::filter(is.na(formant_outlier)) |>
  dplyr::filter(!is.na(log_energy))

# number of rows removed as outliers
nrow(vq_raw) - nrow(vq_clean)

# remove columns that where created
vq_clean <- vq_clean |>
  dplyr::select(-c(f0_outlier, formant_outlier, zF1F2, F0z, Energy_mean))

# Standardization across all pertinate columns. Variables are speaker normalized.

vq_clean <- vq_clean |>
  dplyr::group_by(Talker) |>
  dplyr::mutate(
    dplyr::across(
      .cols = -c(
        Gender,
        Vowel,
        Word,
        Filename,
        Label,
        Phonation,
        seg_Start,
        seg_End
      ),
      .fns = ~ (. - mean(., na.rm = TRUE) / sd(., na.rm = TRUE)),
      .names = "{.col}_z"
    )
  )

# normalize soe
vq_clean <- vq_clean |>
  dplyr::group_by(Talker) |>
  dplyr::mutate(
    log_soe = log10(soe_mean + 0.001),
    m_log_soe = mean(log_soe, na.rm = T),
    sd_log_soe = sd(log_soe, na.rm = T),
    z_log_soe = (log_soe - m_log_soe) / sd_log_soe,
    max_soe = max(log_soe),
    min_soe = min(log_soe),
    soe_norm = (log_soe - min_soe) / (max_soe - min_soe)
  ) |>
  dplyr::select(
    -c(log_soe, m_log_soe, sd_log_soe, z_log_soe, max_soe, min_soe, soe_mean_z)
  ) |>
  dplyr::ungroup()

# Adding function for calculating residual H1*
source(here::here("scripts", "functions", "calc_residH1.R"))

# Applying the function to the dataframe
vq_clean <- vq_clean |>
  calc_residH1(
    h1cz_col = "H1c_mean_z",
    energyz_col = "log_energy_z",
    speaker_col = "Talker"
  )

# uses the dataframe where residual H1* was added.
vq_resid <- vq_clean |>
  select(-H1c_mean_z)

# Factorizing the phonation variable
vq_resid$Phonation <- factor(vq_resid$Phonation)
```

## Data 

The voice quality and classification data comes from `VoicesVQ_data` and `b-c_voice_cluster_labels`. 

`VoicesVQ_data` contains the output of VoiceSauce. 

`b-c_voice_cluster_labels` contains the classification labels that participants assigned to each speaker. 

These data were combined so each line of `VoicesVQ_data` contained the classification label based on subject id. This was stored as a new dataframe called `vq_raw`. This dataframe was cleaned of outliers following Brinekrhoff & McGuire 2025. 

Data was then standardized to make comparisons easier. Energy was first log-transformed to account of its right tail before being z-scored. SoE was normalized following Garellek et al. 2020. H1* was normalized following Chai & Garellek 2022 and replaced z-scored H1*. 

Uncorrected acoustic measures, the non-STRAIGHT f0, and non-snack formants and bandwidths were removed. 

This resulted in a dataframe with the following columns:

  - `Gender`: factor indicating the talkers' gender
  - `Talker`: factor indicating the speakers' id
  - `Vowel`: factor indicating which vowel
  - `Word`: Character
  - `Filename`: character
  - `Label`: character
  - `Phonation`: character
  - `seg_Start`:   numeric
  - `seg_End`:   numeric
  - `H2c_mean_z`:   numeric
  - `H4c_mean_z`:   numeric
  - `A1c_mean_z`:   numeric
  - `A2c_mean_z`:   numeric
  - `A3c_mean_z`:   numeric
  - `H2Kc_mean_z`:   numeric
  - `H1H2c_mean_z`:   numeric
  - `H2H4c_mean_z`:   numeric
  - `H1A1c_mean_z`:   numeric
  - `H1A2c_mean_z`:   numeric
  - `H1A3c_mean_z`:   numeric
  - `H42Kc_mean_z`:   numeric
  - `H2KH5Kc_mean_z`:   numeric
  - `CPP_mean_z`:   numeric
  - `HNR05_mean_z`:   numeric
  - `HNR15_mean_z`:   numeric
  - `HNR25_mean_z`:   numeric
  - `HNR35_mean_z`:   numeric
  - `SHR_mean_z`:   numeric
  - `strF0_mean_z`:   numeric
  - `sF1_mean_z`:   numeric
  - `sF2_mean_z`:   numeric
  - `sF3_mean_z`:   numeric
  - `sF4_mean_z`:   numeric
  - `sB1_mean_z`:   numeric
  - `sB2_mean_z`:   numeric
  - `sB3_mean_z`:   numeric
  - `sB4_mean_z`:   numeric
  - `epoch_mean_z`:   numeric
  - `log_energy_z`:   numeric
  - `soe_norm`:   numeric
  - `resid_H1c`:   numeric

## Analysis
### Preparing for analysis

I removed the columns `Gender`, `Talker`, `Vowel`, `Word`, `Filename`, `Label`,`seg_Start`, and `seg_End` from the dataframe and factorized `Phonation` for Random Forest analysis. 

### Splitting into test and training 
```{r}
#| label: splitting
#| code-fold: true

# stratified sampling with the rsample package with respect to phonation
set.seed(123) # needed for reproducibility
resid_split <- rsample::initial_split(
  vq_resid,
  prop = 0.7,
  strata = "Phonation"
)
resid_train <- rsample::training(resid_split)
resid_test <- rsample::testing(resid_split)
```

Data was randommly split into a testing and training dataset. The proporations of each phonation type was maintained in each dataset. 

```{r}
#| label: tab_training_split
#| fig-cap: "Phonation proporation in the training dataset"

table(resid_train$Phonation) |>
  prop.table()
```
```{r}
#| label: tab_testing_split
#| fig-cap: "Phonation proporation in the testing dataset"

table(resid_test$Phonation) |>
  prop.table()

```


### Tuning the model

Random Forest models need to have the correct parameter settings for the analysis to work. These parameters were chosen based on a grid search. 

```{r}
#| label: hypergrid_search

# determine the number of features
resid_features <- length(setdiff(names(vq_resid), "Phonation"))

# Train a default Random Forest model for comparison
resid_default <- ranger::ranger(
  formula = Phonation ~ .,
  data = resid_train,
  num.trees = resid_features * 100,
  mtry = floor(sqrt(resid_features)),
  respect.unordered.factors = "order",
  seed = 123
)

# default prediction error for comparison
resid_error <- resid_default$prediction.error

# generate hypergrid paramaters
hypergrid_resid <- expand.grid(
  mtry = floor(resid_features * c(.05, .15, .2, .25, .333, .4, 1)),
  min.node.size = c(1, 3, 5, 10),
  replace = c(TRUE, FALSE),
  sample.fraction = c(.5, .63, .8),
  error = NA
)

# execute full grid search
for (i in seq_len(nrow(hypergrid_resid))) {
  # fit the model with the i-th hyperparameter combonation
  fit_resid <- ranger::ranger(
    formula = Phonation ~ .,
    data = resid_train,
    num.trees = resid_features * 100,
    mtry = hypergrid_resid$mtry[i],
    min.node.size = hypergrid_resid$min.node.size[i],
    replace = hypergrid_resid$replace[i],
    sample.fraction = hypergrid_resid$sample.fraction[i],
    respect.unordered.factors = "order",
    seed = 123
  )

  # export OOB RMSE
  hypergrid_resid$error[i] <- fit_resid$prediction.error
}

# assessing the model parameters
hypergrid_resid %>%
  dplyr::arrange(error) %>%
  dplyr::mutate(perc_gain = (resid_error - error) / resid_error * 100) %>%
  head(10)
```

<!-- 
# create a hypergrid for number of trees
hypergrid_resid_trees <- expand.grid(
  num.trees = seq(50, 2000, 50),
  mtry = floor(resid_features * c(.05, .15, .2, .25, .333, .4, 1)),
  accuracy = NA
)

# perform grid search for correct number of trees
for (i in seq_len(nrow(hypergrid_resid_trees))) {
  # fit the model with i-th hyperparameter combination
  fit_resid_trees <- ranger::ranger(
    formula = Phonation ~ .,
    data = resid_train,
    num.trees = hypergrid_resid_trees$num.trees[i],
    mtry = hypergrid_resid_trees$mtry[i],
    min.node.size = 5,
    replace = FALSE,
    sample.fraction = 0.8,
    respect.unordered.factors = "order",
    seed = 123
  )

  # export OOB RMSE
  hypergrid_resid_trees$accuracy[i] <- fit_resid_trees$prediction.error
}

# determine what the best number of trees
hypergrid_resid_trees %>%
  dplyr::arrange(accuracy) %>%
  mutate(perc_gain = (resid_error - accuracy) / resid_error * 100) %>%
  head(10)

# plotting the results
hypergrid_resid_trees %>%
  ggplot(aes(x = num.trees, y = accuracy, color = factor(mtry))) +
  geom_line(linewidth = 1) +
  # geom_line(aes(linetype = factor(mtry)), linewidth = 1) +
  labs(
    title = "Prediction error for Random Forest Hyperparameter Tuning",
    x = "number of trees",
    y = "% incorrect",
    color = "mtry"
  ) +
  scale_color_manual(values = colorblind) +
  theme_bw()

# final random forest model with impurity
resid_final_impurity <- ranger::ranger(
  formula = Phonation ~ .,
  data = resid_train,
  num.trees = 650,
  mtry = 12,
  min.node.size = 5,
  replace = FALSE,
  sample.fraction = 0.8,
  respect.unordered.factors = "order",
  seed = 123,
  classification = TRUE,
  importance = "impurity",
  probability = TRUE
)

# final model with permutation
resid_final_permutation <- ranger::ranger(
  formula = Phonation ~ .,
  data = resid_train,
  num.trees = 650,
  mtry = 12,
  min.node.size = 5,
  replace = FALSE,
  sample.fraction = 0.8,
  respect.unordered.factors = "order",
  seed = 123,
  classification = TRUE,
  importance = "permutation",
  probability = TRUE
)

# Extract variable importance scores for impurity-based importance
resid_impurity_scores <- vip::vi(resid_final_impurity)

# Extract variable importance scores for permutation-based importance
resid_permutation_scores <- vip::vi(resid_final_permutation)

# Create a Lollipop chart of variable importance scores
resid_impurity_plot <- resid_impurity_scores |>
  dplyr::rename_with(~ stringr::str_remove_all(., "_mean_z$|_z$|_norm$")) |>
  ggplot2::ggplot(
    aes(x = reorder(Variable, Importance), y = Importance)
  ) +
  geom_segment(aes(xend = Variable, yend = 0)) +
  geom_point(size = 2) +
  coord_flip() +
  labs(
    title = "Impurity Importance",
    x = "Variable",
    y = "Importance (Impurity)"
  ) +
  theme_bw()
resid_impurity_plot

resid_permutation_plot <- resid_permutation_scores |>
  ggplot2::ggplot(
    aes(x = reorder(Variable, Importance), y = Importance)
  ) +
  geom_segment(aes(xend = Variable, yend = 0)) +
  geom_point(size = 2) +
  coord_flip() +
  labs(
    title = "Permutation Importance",
    x = "Variable",
    y = "Importance (Permutation)"
  ) +
  theme_bw()
resid_permutation_plot

resid_variable_importance_plot <- cowplot::plot_grid(
  resid_impurity_plot,
  resid_permutation_plot,
  nrow = 1
)
resid_variable_importance_plot -->



